{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c331d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai-agents -q --break-system-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zrbug0ogj6c",
   "source": "## Step 1: Environment Setup\n\n### 📦 Installing the OpenAI Agents Library\n\nThe `openai-agents` package is a powerful framework that abstracts complex AI agent operations into simple, intuitive APIs.\n\n#### Key Features:\n- **Agent Management**: Create, configure, and orchestrate multiple agents\n- **Conversation Context**: Maintain state across interactions\n- **Tool Integration**: Connect agents to external functions and APIs\n- **Execution Tracing**: Built-in debugging and monitoring\n- **Async Support**: Native asynchronous operations for scalability\n\n#### Installation Details:\n- **Package**: `openai-agents` - Official OpenAI agent framework\n- **Flag**: `--break-system-packages` - Required for system-wide installation\n- **Mode**: `-q` (quiet) - Suppresses verbose installation output\n\n#### What's Being Installed:\n1. Core agent framework\n2. OpenAI API client\n3. Async utilities\n4. Tracing and debugging tools\n5. Type definitions for better IDE support",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "s4w8xb1dxos",
   "source": "# Week 1: Introduction to AI Agents with OpenAI\n\n## 🎯 Learning Objectives\nBy the end of this notebook, you will:\n- ✅ Understand the fundamentals of AI agent creation and architecture\n- ✅ Master the OpenAI Agents library core components\n- ✅ Create and run your first autonomous AI agent\n- ✅ Implement agent execution with debugging and tracing\n- ✅ Understand asynchronous programming patterns for AI agents\n\n## 📚 Prerequisites\n- Python 3.8+ installed\n- OpenAI API key configured in `.env` file\n- Basic understanding of Python async/await syntax\n- Familiarity with AI/LLM concepts\n\n## 🚀 Project Overview\nThis notebook introduces you to the world of AI agents through a hands-on example. We'll build a \"Jokester\" agent that demonstrates:\n- **Agent initialization** with custom personalities\n- **Asynchronous execution** for optimal performance  \n- **Tracing and debugging** for development\n- **Result handling** and output processing\n\n## 🏗️ What You'll Build\nA simple but complete AI agent system that:\n1. Loads secure API credentials\n2. Creates a joke-telling AI agent\n3. Processes user requests asynchronously\n4. Returns AI-generated jokes with proper error handling\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7141e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The imports\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, trace\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4moilne83xh",
   "source": "## Step 2: Import Required Libraries\n\n### 🔧 Core Components Deep Dive\n\n```python\nfrom dotenv import load_dotenv    # Environment variable management\nfrom agents import Agent, Runner, trace  # Core agent framework\n```\n\n#### Component Breakdown:\n\n##### 1. **`dotenv`** - Security Layer\n- **Purpose**: Loads environment variables from `.env` file\n- **Why It Matters**: Keeps API keys out of source code\n- **Best Practice**: Never commit `.env` files to version control\n\n##### 2. **`Agent`** - The Brain\n- **Purpose**: Defines AI agent behavior and capabilities\n- **Key Features**:\n  - Custom instructions (personality)\n  - Model selection\n  - Tool integration\n  - State management\n\n##### 3. **`Runner`** - The Executor\n- **Purpose**: Manages agent lifecycle and execution\n- **Responsibilities**:\n  - Initialize agent sessions\n  - Handle async operations\n  - Manage conversations\n  - Return results\n\n##### 4. **`trace`** - The Debugger\n- **Purpose**: Provides visibility into agent operations\n- **Benefits**:\n  - Performance monitoring\n  - Error tracking\n  - Execution flow visualization\n  - Development debugging\n\n### 🎨 Architecture Pattern\n```\nUser Input → Runner → Agent → LLM → Response\n                ↑                      ↓\n              Trace ← ← ← ← ← ← ← ← ← ↓\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58a1ff16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The usual starting point\n",
    "\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gfxxvst7ct",
   "source": "## Step 3: Load Environment Variables\n\n### 🔐 Security Configuration\n\n```python\nload_dotenv(override=True)\n```\n\n#### What's Happening:\n1. **Searches** for `.env` file in current and parent directories\n2. **Loads** all key-value pairs as environment variables\n3. **Overrides** existing variables when `override=True`\n4. **Returns** `True` if successful, `False` otherwise\n\n#### Your `.env` File Structure:\n```bash\n# OpenAI Configuration\nOPENAI_API_KEY=sk-...your-key-here...\n\n# Optional: Model Configuration\nOPENAI_MODEL=gpt-4.1-nano-2025-04-14\nOPENAI_TEMPERATURE=0.7\n\n# Optional: Organization Settings\nOPENAI_ORG_ID=org-...your-org-id...\n```\n\n#### Security Best Practices:\n1. **Never commit** `.env` files to Git\n2. **Add to `.gitignore`**: `echo \".env\" >> .gitignore`\n3. **Use `.env.example`** for template sharing\n4. **Rotate keys** regularly\n5. **Use different keys** for dev/staging/production\n\n#### Common Issues & Solutions:\n| Issue | Solution |\n|-------|----------|\n| `False` returned | Check `.env` file exists in correct location |\n| API key not found | Verify key name matches expected variable |\n| Permission denied | Check file permissions: `chmod 600 .env` |\n| Key not working | Verify key validity in OpenAI dashboard |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff872ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make an agent with name, instructions, model\n",
    "\n",
    "agent = Agent(name=\"Jokester\", instructions=\"You are a joke teller\", model=\"gpt-4.1-nano-2025-04-14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pqprav0y28r",
   "source": "## Step 4: Create Your First AI Agent\n\n### 🤖 Agent Architecture & Configuration\n\n```python\nagent = Agent(\n    name=\"Jokester\",                          # Agent identifier\n    instructions=\"You are a joke teller\",     # Behavior definition\n    model=\"gpt-4.1-nano-2025-04-14\"          # LLM selection\n)\n```\n\n#### Core Parameters Explained:\n\n##### 1. **`name`** - Agent Identity\n- **Purpose**: Unique identifier for the agent\n- **Usage**: Logging, debugging, multi-agent systems\n- **Best Practice**: Use descriptive, action-oriented names\n- **Examples**: \"DataAnalyst\", \"CodeReviewer\", \"CustomerSupport\"\n\n##### 2. **`instructions`** - Agent Personality\n- **Purpose**: System prompt that defines behavior\n- **Impact**: Directly shapes response style and content\n- **Tips for Effective Instructions**:\n  ```python\n  # Basic\n  \"You are a joke teller\"\n  \n  # Detailed\n  \"You are a professional comedian who tells clean, \n   family-friendly jokes. You specialize in puns and \n   wordplay. Always be upbeat and positive.\"\n  \n  # Advanced\n  \"You are an expert joke writer who:\n   - Creates original, contextual humor\n   - Adapts style to the audience\n   - Avoids offensive content\n   - Explains jokes if asked\n   - Can discuss comedy theory\"\n  ```\n\n##### 3. **`model`** - LLM Selection\n- **Current**: `gpt-4.1-nano-2025-04-14`\n- **Alternatives**:\n  | Model | Use Case | Cost | Speed |\n  |-------|----------|------|-------|\n  | gpt-4.1-nano | Simple tasks, demos | $ | Fast |\n  | gpt-4o-mini | Balanced performance | $$ | Medium |\n  | gpt-4o | Complex reasoning | $$$ | Slower |\n  | gpt-4-turbo | Latest features | $$$$ | Varies |\n\n#### Advanced Agent Configuration:\n```python\n# Full configuration example\nadvanced_agent = Agent(\n    name=\"ProfessionalJokester\",\n    instructions=\"\"\"You are a professional stand-up comedian with 20 years \n                   of experience. You craft intelligent, witty jokes that are \n                   appropriate for corporate events. You understand timing, \n                   delivery, and audience psychology.\"\"\",\n    model=\"gpt-4o-mini\",\n    temperature=0.8,  # Higher = more creative\n    max_tokens=150,   # Limit response length\n    tools=[],         # Add custom functions later\n    metadata={\"version\": \"1.0\", \"category\": \"entertainment\"}\n)\n```\n\n#### Agent Lifecycle:\n```\nCreation → Configuration → Execution → Response → Cleanup\n    ↓           ↓             ↓           ↓          ↓\n  Memory    Instructions   Runner     Output    Garbage\n  Alloc      Loading      Process   Handling   Collection\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7153ab79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the autonomous AI agent refuse to take a break?  \n",
      "\n",
      "Because it didn’t want to lose momentum—it's always wired to keep moving forward!\n"
     ]
    }
   ],
   "source": [
    "# Run the joke with Runner.run(agent, prompt) then print final_output\n",
    "\n",
    "with trace(\"Telling a joke\"):\n",
    "    result = await Runner.run(agent, \"Tell a joke about Autonomous AI Agents\")\n",
    "    print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ynm3c6lulx",
   "source": "## Step 5: Execute the Agent\n\n### ⚡ Asynchronous Agent Execution\n\n```python\nwith trace(\"Telling a joke\"):\n    result = await Runner.run(agent, \"Tell a joke about Autonomous AI Agents\")\n    print(result.final_output)\n```\n\n#### Execution Flow Breakdown:\n\n##### 1. **Tracing Context** - `with trace(\"Telling a joke\")`\n- **Purpose**: Wraps execution for monitoring\n- **Features**:\n  - Performance metrics (latency, tokens used)\n  - Error capture and reporting\n  - Execution path visualization\n  - Debug information collection\n- **Output**: Trace ID for debugging\n- **Best Practice**: Always use descriptive trace names\n\n##### 2. **Runner Execution** - `Runner.run()`\n- **Async Operation**: Uses `await` for non-blocking execution\n- **Parameters**:\n  ```python\n  result = await Runner.run(\n      agent,              # The agent to execute\n      prompt,             # User input/question\n      context={},         # Optional: Previous context\n      stream=False,       # Optional: Enable streaming\n      timeout=30,         # Optional: Timeout in seconds\n      retry_count=3       # Optional: Auto-retry on failure\n  )\n  ```\n\n##### 3. **Result Object Structure**:\n```python\nresult = {\n    'final_output': str,      # The agent's response\n    'usage': {                # Token usage stats\n        'prompt_tokens': int,\n        'completion_tokens': int,\n        'total_tokens': int\n    },\n    'metadata': {             # Execution metadata\n        'model': str,\n        'temperature': float,\n        'execution_time': float\n    },\n    'trace_id': str,          # For debugging\n    'success': bool           # Execution status\n}\n```\n\n#### Async/Await Explained:\n```python\n# Synchronous (Blocking) - Bad\ndef get_joke():\n    response = api_call()  # Blocks entire program\n    return response\n\n# Asynchronous (Non-blocking) - Good\nasync def get_joke():\n    response = await api_call()  # Other code can run\n    return response\n```\n\n#### Error Handling Patterns:\n```python\n# Basic error handling\ntry:\n    with trace(\"Joke Generation\"):\n        result = await Runner.run(agent, prompt)\n        print(result.final_output)\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n# Advanced error handling\ntry:\n    with trace(\"Joke Generation\") as t:\n        result = await Runner.run(agent, prompt)\n        if result.success:\n            print(result.final_output)\n        else:\n            print(f\"Failed: {result.error}\")\nexcept TimeoutError:\n    print(\"Request timed out\")\nexcept RateLimitError:\n    print(\"Rate limit exceeded, retry later\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n    print(f\"Trace ID for debugging: {t.trace_id}\")\n```\n\n#### Performance Optimization:\n```python\n# Parallel execution for multiple prompts\nimport asyncio\n\nasync def get_multiple_jokes(topics):\n    tasks = [\n        Runner.run(agent, f\"Tell a joke about {topic}\")\n        for topic in topics\n    ]\n    results = await asyncio.gather(*tasks)\n    return [r.final_output for r in results]\n\n# Usage\ntopics = [\"AI\", \"Python\", \"Cloud Computing\"]\njokes = await get_multiple_jokes(topics)\n```\n\n#### Output Example:\n```\nInput: \"Tell a joke about Autonomous AI Agents\"\nOutput: \"Why did the autonomous AI agent refuse to take a break?\n         Because it didn't want to lose momentum—it's always \n         wired to keep moving forward!\"\n```\n\n#### Debugging with Trace:\n```python\n# Enable verbose tracing\nwith trace(\"Detailed Joke Generation\", verbose=True) as t:\n    result = await Runner.run(agent, prompt)\n    \n    # Access trace details\n    print(f\"Trace ID: {t.trace_id}\")\n    print(f\"Duration: {t.duration_ms}ms\")\n    print(f\"Tokens used: {t.token_count}\")\n    print(f\"Model: {t.model}\")\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57caa82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8kr8ot2wt6v",
   "source": "---\n\n## 📊 Summary & Key Concepts\n\n### What You've Accomplished:\n✅ **Set up** a secure AI agent development environment  \n✅ **Created** your first AI agent with custom personality  \n✅ **Executed** asynchronous agent operations  \n✅ **Implemented** proper tracing and debugging  \n✅ **Generated** AI-powered content (jokes)  \n\n### 🧠 Core Concepts Mastered:\n\n#### 1. **Agent Architecture**\n```\nUser Input → Agent Instructions → LLM Processing → Formatted Output\n```\n\n#### 2. **Asynchronous Programming**\n- Non-blocking execution for better performance\n- Concurrent processing capabilities\n- Proper error handling with try/except\n\n#### 3. **Security Best Practices**\n- Environment variables for sensitive data\n- API key management\n- Secure configuration patterns\n\n### 🚀 Next Steps & Exercises:\n\n#### Beginner Exercises:\n1. **Personality Variations**: Create agents with different joke styles (puns, dark humor, dad jokes)\n2. **Topic Specialization**: Build a joke agent that only tells tech jokes\n3. **Response Length**: Control joke length with max_tokens parameter\n\n#### Intermediate Challenges:\n1. **Multi-Agent System**: Create multiple agents that tell jokes in a conversation\n2. **Context Memory**: Build an agent that remembers previous jokes\n3. **Streaming Responses**: Implement real-time streaming for longer outputs\n\n#### Advanced Projects:\n1. **Joke Rating System**: Add a tool that rates joke quality\n2. **Audience Adaptation**: Agent adjusts humor based on audience profile\n3. **Joke Database**: Store and retrieve best jokes for reuse\n\n### 💡 Pro Tips:\n\n1. **Optimize Costs**: Use smaller models for simple tasks\n2. **Cache Results**: Store common responses to reduce API calls\n3. **Monitor Usage**: Track token consumption for budget control\n4. **Test Locally**: Use mock responses during development\n5. **Version Control**: Track agent instruction changes\n\n### 🔧 Troubleshooting Guide:\n\n| Problem | Solution |\n|---------|----------|\n| Agent not responding | Check API key and internet connection |\n| Slow responses | Use faster model or implement caching |\n| Inconsistent output | Adjust temperature parameter |\n| Rate limits | Implement exponential backoff |\n| High costs | Optimize prompts and use smaller models |\n\n### 📚 Resources for Continued Learning:\n\n- **OpenAI Documentation**: [platform.openai.com/docs](https://platform.openai.com/docs)\n- **Agent Frameworks**: Explore LangChain, AutoGPT, CrewAI\n- **Best Practices**: OpenAI's prompt engineering guide\n- **Community**: Join AI agent development forums\n\n### 🎯 Key Takeaways:\n\n1. **Agents are configurable AI entities** with specific behaviors\n2. **The Runner manages execution** and handles async operations\n3. **Tracing provides visibility** into agent performance\n4. **Asynchronous execution** enables scalable applications\n5. **Security is paramount** - protect your API keys\n\n### 💬 Final Thoughts:\n\nYou've just scratched the surface of AI agent development! This simple joke-telling agent demonstrates fundamental concepts that scale to complex, production-ready systems. The same patterns you've learned here apply to:\n\n- Customer service agents\n- Code review assistants\n- Data analysis tools\n- Content generation systems\n- Automated research agents\n\nKeep experimenting, and remember: the best way to learn is by building!\n\n---\n\n**Ready for Week 2?** Next, we'll explore multi-agent systems, tool integration, and advanced orchestration patterns!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}